"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[39],{8453:(n,e,t)=>{t.d(e,{R:()=>r,x:()=>s});var i=t(6540);const a={},o=i.createContext(a);function r(n){const e=i.useContext(o);return i.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),i.createElement(o.Provider,{value:e},n.children)}},9123:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-03/rl-for-humanoid-control","title":"Reinforcement Learning for Humanoid Control","description":"Introduction to Reinforcement Learning in Robotics","source":"@site/docs/module-03/15-rl-for-humanoid-control.md","sourceDirName":"module-03","slug":"/module-03/rl-for-humanoid-control","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-03/rl-for-humanoid-control","draft":false,"unlisted":false,"editUrl":"https://github.com/alismehtab7-125/Physical-AI-Humanoid-Robotics/edit/main/docs/module-03/15-rl-for-humanoid-control.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Reinforcement Learning for Humanoid Control"},"sidebar":"tutorialSidebar","previous":{"title":"Advanced Sensor Fusion and Perception","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-03/sensor-fusion-and-perception"},"next":{"title":"Module 4 - Vision-Language-Action (V-L-A)","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/"}}');var a=t(4848),o=t(8453);const r={sidebar_position:5,title:"Reinforcement Learning for Humanoid Control"},s="Reinforcement Learning for Humanoid Control",l={},c=[{value:"Introduction to Reinforcement Learning in Robotics",id:"introduction-to-reinforcement-learning-in-robotics",level:2},{value:"The RL Framework for Humanoid Control",id:"the-rl-framework-for-humanoid-control",level:3},{value:"RL Algorithms for Humanoid Control",id:"rl-algorithms-for-humanoid-control",level:2},{value:"Proximal Policy Optimization (PPO)",id:"proximal-policy-optimization-ppo",level:3},{value:"Soft Actor-Critic (SAC)",id:"soft-actor-critic-sac",level:3},{value:"Isaac Sim for RL Training",id:"isaac-sim-for-rl-training",level:2},{value:"Creating RL Training Environments",id:"creating-rl-training-environments",level:3},{value:"Integration with RL Libraries",id:"integration-with-rl-libraries",level:3},{value:"Sim-to-Real Transfer for RL Policies",id:"sim-to-real-transfer-for-rl-policies",level:2},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"System Identification and Model Adaptation",id:"system-identification-and-model-adaptation",level:3},{value:"Advanced RL Techniques for Humanoid Control",id:"advanced-rl-techniques-for-humanoid-control",level:2},{value:"Hierarchical RL",id:"hierarchical-rl",level:3},{value:"Curriculum Learning",id:"curriculum-learning",level:3},{value:"Isaac Sim RL Best Practices",id:"isaac-sim-rl-best-practices",level:2},{value:"1. Reward Shaping",id:"1-reward-shaping",level:3},{value:"2. Simulation Parameters",id:"2-simulation-parameters",level:3},{value:"3. Parallel Training",id:"3-parallel-training",level:3},{value:"Challenges and Solutions in RL for Humanoid Control",id:"challenges-and-solutions-in-rl-for-humanoid-control",level:2},{value:"1. Sample Efficiency",id:"1-sample-efficiency",level:3},{value:"2. Safety During Training",id:"2-safety-during-training",level:3},{value:"3. Sim-to-Real Transfer",id:"3-sim-to-real-transfer",level:3}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"reinforcement-learning-for-humanoid-control",children:"Reinforcement Learning for Humanoid Control"})}),"\n",(0,a.jsx)(e.h2,{id:"introduction-to-reinforcement-learning-in-robotics",children:"Introduction to Reinforcement Learning in Robotics"}),"\n",(0,a.jsx)(e.p,{children:"Reinforcement Learning (RL) has emerged as a powerful paradigm for learning complex control policies in robotics, particularly for humanoid robots that require sophisticated locomotion and manipulation skills. Unlike traditional control methods that rely on hand-designed controllers, RL enables robots to learn optimal behaviors through interaction with their environment, making it ideal for tasks that are difficult to engineer explicitly."}),"\n",(0,a.jsx)(e.h3,{id:"the-rl-framework-for-humanoid-control",children:"The RL Framework for Humanoid Control"}),"\n",(0,a.jsx)(e.p,{children:"In the RL framework, a humanoid robot (the agent) interacts with its environment by taking actions and receiving rewards based on its performance. The goal is to learn a policy that maximizes cumulative rewards over time:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"State (s)"}),": Robot's configuration, joint positions, velocities, sensor readings, and environmental information"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action (a)"}),": Joint torques, positions, or velocities to control the robot"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Reward (r)"}),": Feedback signal that guides learning (e.g., forward velocity, balance maintenance, task completion)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Policy (\u03c0)"}),": Mapping from states to actions that the robot learns to optimize"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"rl-algorithms-for-humanoid-control",children:"RL Algorithms for Humanoid Control"}),"\n",(0,a.jsx)(e.h3,{id:"proximal-policy-optimization-ppo",children:"Proximal Policy Optimization (PPO)"}),"\n",(0,a.jsx)(e.p,{children:"PPO is one of the most successful on-policy RL algorithms for humanoid control, known for its stability and sample efficiency:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\n\nclass PPOActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(PPOActorCritic, self).__init__()\n\n        # Actor network (policy)\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, action_dim),\n        )\n\n        # Critic network (value function)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.Tanh(),\n            nn.Linear(hidden_dim, 1),\n        )\n\n    def forward(self, state):\n        action_mean = self.actor(state)\n        value = self.critic(state)\n        return action_mean, value\n\nclass PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, clip_epsilon=0.2):\n        self.actor_critic = PPOActorCritic(state_dim, action_dim)\n        self.optimizer = optim.Adam(self.actor_critic.parameters(), lr=lr)\n        self.clip_epsilon = clip_epsilon\n\n    def select_action(self, state, deterministic=False):\n        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n        action_mean, value = self.actor_critic(state_tensor)\n\n        if deterministic:\n            return action_mean.detach().numpy()[0], value.detach().numpy()[0]\n\n        # Add Gaussian noise for exploration\n        action_std = torch.ones_like(action_mean) * 0.1\n        action = action_mean + torch.randn_like(action_mean) * action_std\n        return action.detach().numpy()[0], value.detach().numpy()[0]\n\n    def update(self, states, actions, old_log_probs, returns, advantages):\n        states = torch.FloatTensor(states)\n        actions = torch.FloatTensor(actions)\n        old_log_probs = torch.FloatTensor(old_log_probs)\n        returns = torch.FloatTensor(returns).unsqueeze(1)\n        advantages = torch.FloatTensor(advantages)\n\n        # Current policy\n        new_action_means, new_values = self.actor_critic(states)\n        new_log_probs = self.gaussian_log_prob(new_action_means, actions)\n\n        # Policy ratio\n        ratio = torch.exp(new_log_probs - old_log_probs)\n\n        # PPO objective\n        surr1 = ratio * advantages\n        surr2 = torch.clamp(ratio, 1 - self.clip_epsilon, 1 + self.clip_epsilon) * advantages\n        actor_loss = -torch.min(surr1, surr2).mean()\n\n        # Value loss\n        value_loss = nn.MSELoss()(new_values, returns)\n\n        # Total loss\n        total_loss = actor_loss + 0.5 * value_loss\n\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()\n\n    def gaussian_log_prob(self, means, samples):\n        # Calculate log probability of Gaussian distribution\n        log_probs = -0.5 * ((samples - means) ** 2).sum(dim=-1)\n        return log_probs\n"})}),"\n",(0,a.jsx)(e.h3,{id:"soft-actor-critic-sac",children:"Soft Actor-Critic (SAC)"}),"\n",(0,a.jsx)(e.p,{children:"SAC is an off-policy algorithm that maximizes both reward and entropy, leading to more robust policies:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nfrom collections import deque\n\nclass SACActor(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(SACActor, self).__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n        )\n\n        self.mean_layer = nn.Linear(hidden_dim, action_dim)\n        self.log_std_layer = nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, state):\n        x = self.net(state)\n        mean = self.mean_layer(x)\n        log_std = self.log_std_layer(x)\n        log_std = torch.clamp(log_std, -20, 2)\n        return mean, log_std\n\nclass SACCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(SACCritic, self).__init__()\n\n        self.q1 = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n        self.q2 = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n\n    def forward(self, state, action):\n        sa = torch.cat([state, action], dim=1)\n        q1 = self.q1(sa)\n        q2 = self.q2(sa)\n        return q1, q2\n\nclass SACAgent:\n    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, tau=0.005):\n        self.actor = SACActor(state_dim, action_dim)\n        self.critic = SACCritic(state_dim, action_dim)\n        self.target_critic = SACCritic(state_dim, action_dim)\n\n        # Copy critic weights to target critic\n        self.target_critic.load_state_dict(self.critic.state_dict())\n\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n\n        self.gamma = gamma\n        self.tau = tau\n        self.target_entropy = -action_dim\n        self.log_alpha = torch.zeros(1, requires_grad=True)\n        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n\n    def select_action(self, state, evaluate=False):\n        state = torch.FloatTensor(state).unsqueeze(0)\n        mean, log_std = self.actor(state)\n        std = log_std.exp()\n\n        if evaluate:\n            return mean.detach().numpy()[0]\n\n        # Reparameterization trick\n        normal = torch.distributions.Normal(mean, std)\n        x_t = normal.rsample()\n        action = torch.tanh(x_t)\n        log_prob = normal.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)\n        log_prob = log_prob.sum(dim=1, keepdim=True)\n\n        return action.detach().numpy()[0]\n\n    def update(self, replay_buffer, batch_size=256):\n        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n\n        state = torch.FloatTensor(state)\n        action = torch.FloatTensor(action)\n        reward = torch.FloatTensor(reward).unsqueeze(1)\n        next_state = torch.FloatTensor(next_state)\n        done = torch.FloatTensor(done).unsqueeze(1)\n\n        # Update Q functions\n        with torch.no_grad():\n            next_action, next_log_prob = self.actor(next_state)\n            next_q1, next_q2 = self.target_critic(next_state, next_action)\n            next_q = torch.min(next_q1, next_q2) - self.log_alpha * next_log_prob\n            target_q = reward + (1 - done) * self.gamma * next_q\n\n        current_q1, current_q2 = self.critic(state, action)\n        q_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)\n\n        self.critic_optimizer.zero_grad()\n        q_loss.backward()\n        self.critic_optimizer.step()\n\n        # Update policy\n        new_actions, log_prob = self.actor(state)\n        q1, q2 = self.critic(state, new_actions)\n        q = torch.min(q1, q2)\n        policy_loss = (self.log_alpha * log_prob - q).mean()\n\n        self.actor_optimizer.zero_grad()\n        policy_loss.backward()\n        self.actor_optimizer.step()\n\n        # Update temperature\n        alpha_loss = -(self.log_alpha * (log_prob + self.target_entropy).detach()).mean()\n        self.alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optimizer.step()\n\n        # Update target networks\n        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n"})}),"\n",(0,a.jsx)(e.h2,{id:"isaac-sim-for-rl-training",children:"Isaac Sim for RL Training"}),"\n",(0,a.jsx)(e.h3,{id:"creating-rl-training-environments",children:"Creating RL Training Environments"}),"\n",(0,a.jsx)(e.p,{children:"Isaac Sim provides comprehensive tools for creating RL training environments for humanoid robots:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Isaac Sim RL environment for humanoid locomotion\nimport omni\nfrom omni.isaac.core import World\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.core.utils.prims import get_prim_at_path\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.tasks import BaseTask\nfrom pxr import Gf\nimport numpy as np\n\nclass HumanoidLocomotionTask(BaseTask):\n    def __init__(\n        self,\n        name,\n        offset=None,\n        target_velocity=1.0,\n        max_episode_length=1000\n    ):\n        super().__init__(name=name, offset=offset)\n        self._num_envs = 1\n        self._target_velocity = target_velocity\n        self._max_episode_length = max_episode_length\n        self._current_step = 0\n\n    def set_up_scene(self, scene) -> None:\n        # Add ground plane\n        super().set_up_scene(scene)\n        scene.add_default_ground_plane()\n\n        # Add humanoid robot\n        self._robot = Robot(\n            prim_path="/World/Robot",\n            name="humanoid_robot",\n            usd_path="/path/to/humanoid_model.usd",\n            position=np.array([0.0, 0.0, 1.0])\n        )\n        scene.add(self._robot)\n\n    def get_observations(self) -> dict:\n        # Get robot state information\n        positions = self._robot.get_world_poses(clone=True)[0]\n        orientations = self._robot.get_world_poses(clone=True)[1]\n        joint_positions = self._robot.get_joint_positions()\n        joint_velocities = self._robot.get_joint_velocities()\n\n        # Create observation vector\n        observation = {\n            "joint_positions": joint_positions,\n            "joint_velocities": joint_velocities,\n            "base_position": positions[0],\n            "base_orientation": orientations[0],\n            "target_velocity": self._target_velocity\n        }\n\n        return observation\n\n    def pre_physics_step(self, actions) -> None:\n        # Apply actions to robot\n        if actions is not None:\n            self._robot.set_joint_position_targets(actions)\n\n    def get_rewards(self) -> dict:\n        # Calculate reward based on forward velocity and stability\n        current_velocity = self._calculate_forward_velocity()\n        stability_penalty = self._calculate_stability_penalty()\n\n        # Reward for forward motion\n        velocity_reward = current_velocity * 10.0\n\n        # Penalty for falling\n        fall_penalty = self._check_fall() * -100.0\n\n        # Penalty for instability\n        stability_reward = -stability_penalty * 5.0\n\n        total_reward = velocity_reward + stability_reward + fall_penalty\n        return {"total_reward": total_reward}\n\n    def is_done(self) -> dict:\n        # Check if episode is done\n        done = self._check_fall() or (self._current_step >= self._max_episode_length)\n        return {"done": done}\n\n    def _calculate_forward_velocity(self):\n        # Calculate robot\'s forward velocity\n        current_pos = self._robot.get_world_poses(clone=True)[0][0]\n        if hasattr(self, \'_prev_pos\'):\n            velocity = (current_pos[0] - self._prev_pos[0]) / self._world.get_physics_dt()\n            self._prev_pos = current_pos\n            return velocity\n        else:\n            self._prev_pos = current_pos\n            return 0.0\n\n    def _calculate_stability_penalty(self):\n        # Calculate penalty for unstable posture\n        orientation = self._robot.get_world_poses(clone=True)[1][0]\n        # Check if robot is upright (simplified)\n        up_vector = Gf.Quatf(orientation[6], orientation[3], orientation[4], orientation[5]).Rotate(Gf.Vec3f(0, 0, 1))\n        stability = abs(up_vector[2] - 1.0)  # 0 when upright, higher when tilted\n        return stability\n\n    def _check_fall(self):\n        # Check if robot has fallen\n        base_pos = self._robot.get_world_poses(clone=True)[0][0]\n        return base_pos[2] < 0.5  # Fallen if base is too low\n'})}),"\n",(0,a.jsx)(e.h3,{id:"integration-with-rl-libraries",children:"Integration with RL Libraries"}),"\n",(0,a.jsx)(e.p,{children:"Isaac Sim can be integrated with popular RL libraries like Stable-Baselines3:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'from stable_baselines3 import PPO\nfrom stable_baselines3.ppo import MlpPolicy\nfrom stable_baselines3.common.env_util import make_vec_env\nfrom stable_baselines3.common.callbacks import EvalCallback\nimport gym\nfrom gym import spaces\n\nclass IsaacHumanoidEnv(gym.Env):\n    def __init__(self):\n        super(IsaacHumanoidEnv, self).__init__()\n\n        # Define action and observation spaces\n        self.action_space = spaces.Box(\n            low=-1.0, high=1.0, shape=(19,), dtype=np.float32  # 19 joint torques\n        )\n        self.observation_space = spaces.Box(\n            low=-np.inf, high=np.inf, shape=(60,), dtype=np.float32  # State vector\n        )\n\n        # Initialize Isaac Sim world\n        self.world = World(stage_units_in_meters=1.0)\n        self.task = HumanoidLocomotionTask(name="humanoid_task")\n        self.world.add_task(self.task)\n        self.world.reset()\n\n        self.episode_step = 0\n        self.max_episode_steps = 1000\n\n    def reset(self):\n        self.world.reset()\n        self.episode_step = 0\n        obs = self._get_observation()\n        return obs\n\n    def step(self, action):\n        # Apply action to robot\n        self.world.step(render=True)\n        self.task.pre_physics_step(action)\n\n        # Get observation and reward\n        obs = self._get_observation()\n        reward = self._get_reward()\n        done = self._is_done()\n\n        self.episode_step += 1\n        info = {}\n\n        return obs, reward, done, info\n\n    def _get_observation(self):\n        # Get current robot state as observation\n        obs_dict = self.task.get_observations()\n        # Flatten and normalize observation\n        obs = np.concatenate([\n            obs_dict["joint_positions"],\n            obs_dict["joint_velocities"],\n            obs_dict["base_position"],\n            obs_dict["base_orientation"]\n        ])\n        return obs\n\n    def _get_reward(self):\n        # Get reward from task\n        rewards = self.task.get_rewards()\n        return rewards["total_reward"]\n\n    def _is_done(self):\n        # Check if episode is done\n        done_dict = self.task.is_done()\n        return done_dict["done"] or self.episode_step >= self.max_episode_steps\n\n# Train the humanoid robot using Isaac Sim\ndef train_humanoid():\n    # Create Isaac Sim environment\n    env = IsaacHumanoidEnv()\n\n    # Create PPO agent\n    model = PPO(\n        MlpPolicy,\n        env,\n        verbose=1,\n        tensorboard_log="./logs/",\n        learning_rate=3e-4,\n        n_steps=2048,\n        batch_size=64,\n        n_epochs=10,\n        gamma=0.99,\n        gae_lambda=0.95,\n        clip_range=0.2,\n        ent_coef=0.01\n    )\n\n    # Train the model\n    model.learn(total_timesteps=1000000)\n\n    # Save the trained model\n    model.save("humanoid_policy")\n\n    # Test the trained policy\n    obs = env.reset()\n    for i in range(1000):\n        action, _states = model.predict(obs)\n        obs, rewards, dones, info = env.step(action)\n        if dones:\n            obs = env.reset()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"sim-to-real-transfer-for-rl-policies",children:"Sim-to-Real Transfer for RL Policies"}),"\n",(0,a.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsx)(e.p,{children:"Domain randomization is crucial for successful Sim-to-Real transfer:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class DomainRandomization:\n    def __init__(self, env):\n        self.env = env\n        self.randomization_params = {\n            \'mass_range\': [0.8, 1.2],  # 80% to 120% of original mass\n            \'friction_range\': [0.5, 1.5],  # Friction coefficient range\n            \'actuator_range\': [0.9, 1.1],  # Actuator strength range\n            \'sensor_noise_range\': [0.001, 0.01],  # Sensor noise range\n        }\n\n    def randomize_environment(self):\n        """Randomize environment parameters for domain randomization"""\n        # Randomize robot masses\n        self.randomize_masses()\n\n        # Randomize friction coefficients\n        self.randomize_friction()\n\n        # Randomize actuator properties\n        self.randomize_actuators()\n\n        # Randomize sensor noise\n        self.randomize_sensors()\n\n    def randomize_masses(self):\n        """Randomize link masses"""\n        for link in self.env.robot.links:\n            original_mass = link.mass\n            random_factor = np.random.uniform(\n                self.randomization_params[\'mass_range\'][0],\n                self.randomization_params[\'mass_range\'][1]\n            )\n            link.mass = original_mass * random_factor\n\n    def randomize_friction(self):\n        """Randomize friction coefficients"""\n        for joint in self.env.robot.joints:\n            random_factor = np.random.uniform(\n                self.randomization_params[\'friction_range\'][0],\n                self.randomization_params[\'friction_range\'][1]\n            )\n            joint.friction = joint.original_friction * random_factor\n\n    def apply_randomization(self, step):\n        """Apply randomization periodically during training"""\n        if step % 1000 == 0:  # Randomize every 1000 steps\n            self.randomize_environment()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"system-identification-and-model-adaptation",children:"System Identification and Model Adaptation"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class SystemIdentifier:\n    def __init__(self, sim_env, real_robot):\n        self.sim_env = sim_env\n        self.real_robot = real_robot\n        self.sim_params = {}\n        self.real_params = {}\n        self.adaptation_model = None\n\n    def identify_system(self):\n        """Identify system parameters from real robot data"""\n        # Collect data from real robot\n        real_data = self.collect_real_data()\n\n        # Identify parameters\n        self.real_params = self.estimate_parameters(real_data)\n\n        # Update simulation to match real robot\n        self.update_simulation_params()\n\n        return self.real_params\n\n    def collect_real_data(self):\n        """Collect data from real robot for system identification"""\n        data = []\n\n        # Apply various inputs to real robot\n        for input_sequence in self.generate_excitation_signals():\n            # Apply input and collect response\n            response = self.real_robot.apply_input(input_sequence)\n            data.append((input_sequence, response))\n\n        return data\n\n    def estimate_parameters(self, data):\n        """Estimate system parameters from collected data"""\n        # Use system identification techniques\n        # (e.g., least squares, maximum likelihood, etc.)\n\n        estimated_params = {}\n\n        # Estimate mass properties\n        estimated_params[\'mass\'] = self.estimate_mass_properties(data)\n\n        # Estimate friction parameters\n        estimated_params[\'friction\'] = self.estimate_friction(data)\n\n        # Estimate actuator dynamics\n        estimated_params[\'actuators\'] = self.estimate_actuator_dynamics(data)\n\n        return estimated_params\n\n    def update_simulation_params(self):\n        """Update simulation parameters to match real robot"""\n        for param_name, param_value in self.real_params.items():\n            self.sim_env.set_parameter(param_name, param_value)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"advanced-rl-techniques-for-humanoid-control",children:"Advanced RL Techniques for Humanoid Control"}),"\n",(0,a.jsx)(e.h3,{id:"hierarchical-rl",children:"Hierarchical RL"}),"\n",(0,a.jsx)(e.p,{children:"Hierarchical RL can break down complex humanoid tasks into manageable sub-tasks:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class HierarchicalRL:\n    def __init__(self, low_level_agent, high_level_agent):\n        self.low_level_agent = low_level_agent  # Motor control\n        self.high_level_agent = high_level_agent  # Task planning\n        self.subtask_duration = 100  # Steps per subtask\n\n    def get_action(self, state, step):\n        # High-level agent selects subtask every N steps\n        if step % self.subtask_duration == 0:\n            self.current_subtask = self.high_level_agent.select_subtask(state)\n\n        # Low-level agent executes subtask-specific policy\n        action = self.low_level_agent.execute_subtask(\n            state, self.current_subtask\n        )\n\n        return action\n"})}),"\n",(0,a.jsx)(e.h3,{id:"curriculum-learning",children:"Curriculum Learning"}),"\n",(0,a.jsx)(e.p,{children:"Gradually increasing task difficulty during training:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"class CurriculumLearning:\n    def __init__(self):\n        self.current_level = 0\n        self.difficulty_thresholds = [0.5, 0.7, 0.85, 1.0]  # Performance thresholds\n        self.curriculum_tasks = [\n            'stand_still',\n            'walk_forward',\n            'walk_turn',\n            'navigate_obstacles',\n            'complex_maneuvers'\n        ]\n\n    def update_curriculum(self, performance):\n        \"\"\"Update curriculum based on performance\"\"\"\n        if (self.current_level < len(self.difficulty_thresholds) and\n            performance >= self.difficulty_thresholds[self.current_level]):\n            self.current_level += 1\n            print(f\"Advancing to curriculum level: {self.curriculum_tasks[self.current_level]}\")\n\n        return self.curriculum_tasks[self.current_level]\n"})}),"\n",(0,a.jsx)(e.h2,{id:"isaac-sim-rl-best-practices",children:"Isaac Sim RL Best Practices"}),"\n",(0,a.jsx)(e.h3,{id:"1-reward-shaping",children:"1. Reward Shaping"}),"\n",(0,a.jsx)(e.p,{children:"Design reward functions that guide learning effectively:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def design_reward_function(robot_state, target_state, action):\n    """Well-designed reward function for humanoid control"""\n    reward = 0.0\n\n    # Forward velocity reward\n    forward_vel = calculate_forward_velocity(robot_state)\n    reward += forward_vel * 10.0\n\n    # Energy efficiency penalty\n    energy_cost = np.sum(np.abs(action)) * 0.01\n    reward -= energy_cost\n\n    # Stability reward (keeping torso upright)\n    torso_orientation = get_torso_orientation(robot_state)\n    stability_bonus = max(0, torso_orientation[2]) * 5.0  # Z-axis up\n    reward += stability_bonus\n\n    # Smoothness penalty (action differences)\n    if hasattr(self, \'prev_action\'):\n        action_smoothness = np.sum(np.abs(action - self.prev_action)) * 0.1\n        reward -= action_smoothness\n    self.prev_action = action\n\n    # Survival bonus (staying upright)\n    if not is_fallen(robot_state):\n        reward += 1.0\n\n    return reward\n'})}),"\n",(0,a.jsx)(e.h3,{id:"2-simulation-parameters",children:"2. Simulation Parameters"}),"\n",(0,a.jsx)(e.p,{children:"Configure Isaac Sim for optimal RL training:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Isaac Sim configuration for RL training\nrl_training_config = {\n    "physics_dt": 1.0/240.0,  # Physics simulation timestep\n    "rendering_dt": 1.0/60.0,  # Rendering timestep\n    "max_gpu_contact_pairs": 1000000,\n    "default_physics_material": {\n        "static_friction": 1.0,\n        "dynamic_friction": 1.0,\n        "restitution": 0.0\n    },\n    "gpu_sim": True,  # Use GPU for physics simulation\n    "use_flatcache": True,  # Optimize for large scenes\n    "enable_scene_query_support": True  # For collision detection\n}\n'})}),"\n",(0,a.jsx)(e.h3,{id:"3-parallel-training",children:"3. Parallel Training"}),"\n",(0,a.jsx)(e.p,{children:"Scale RL training using multiple simulation environments:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Parallel environments for faster training\ndef create_parallel_envs(num_envs=16):\n    """Create multiple parallel Isaac Sim environments"""\n    envs = []\n\n    for i in range(num_envs):\n        env = IsaacHumanoidEnv()\n        env.set_offset([i * 5.0, 0, 0])  # Space environments apart\n        envs.append(env)\n\n    return envs\n\n# Use vectorized environments with RL libraries\nfrom stable_baselines3.common.vec_env import SubprocVecEnv\nfrom stable_baselines3.common.env_util import make_vec_env\n\n# Create multiple environments\ndef make_env(rank, seed=0):\n    def _init():\n        env = IsaacHumanoidEnv()\n        env.seed(seed + rank)\n        return env\n    return _init\n\nenv = SubprocVecEnv([make_env(i) for i in range(8)])  # 8 parallel environments\n'})}),"\n",(0,a.jsx)(e.h2,{id:"challenges-and-solutions-in-rl-for-humanoid-control",children:"Challenges and Solutions in RL for Humanoid Control"}),"\n",(0,a.jsx)(e.h3,{id:"1-sample-efficiency",children:"1. Sample Efficiency"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Use domain randomization to improve generalization"}),"\n",(0,a.jsx)(e.li,{children:"Implement curriculum learning for gradual skill acquisition"}),"\n",(0,a.jsx)(e.li,{children:"Leverage prior knowledge through imitation learning"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"2-safety-during-training",children:"2. Safety During Training"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Implement safety constraints in simulation"}),"\n",(0,a.jsx)(e.li,{children:"Use safe exploration techniques"}),"\n",(0,a.jsx)(e.li,{children:"Monitor and limit joint limits and forces"}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"3-sim-to-real-transfer",children:"3. Sim-to-Real Transfer"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Carefully calibrate simulation parameters"}),"\n",(0,a.jsx)(e.li,{children:"Use system identification techniques"}),"\n",(0,a.jsx)(e.li,{children:"Validate policies in real-world conditions gradually"}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Reinforcement Learning provides a powerful approach to learning complex humanoid control policies, enabling robots to acquire sophisticated locomotion and manipulation skills through interaction with their environment. When combined with Isaac Sim's high-fidelity simulation capabilities, RL can significantly accelerate the development of capable humanoid robots."})]})}function m(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}}}]);