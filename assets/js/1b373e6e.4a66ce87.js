"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[231],{294:(n,e,a)=>{a.r(e),a.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"module-04/navigating-complex-environments","title":"Humanoid Navigation in Complex Environments","description":"Introduction to Humanoid Navigation Challenges","source":"@site/docs/module-04/17-navigating-complex-environments.md","sourceDirName":"module-04","slug":"/module-04/navigating-complex-environments","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/navigating-complex-environments","draft":false,"unlisted":false,"editUrl":"https://github.com/alismehtab7-125/Physical-AI-Humanoid-Robotics/edit/main/docs/module-04/17-navigating-complex-environments.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Humanoid Navigation in Complex Environments"},"sidebar":"tutorialSidebar","previous":{"title":"Vision-Language Models (VLM) for Task Planning","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/introduction-to-vlm-llm"},"next":{"title":"High-Level Task Planning using LLMs","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/high-level-task-planning"}}');var t=a(4848),i=a(8453);const s={sidebar_position:2,title:"Humanoid Navigation in Complex Environments"},r="Humanoid Navigation in Complex Environments",l={},c=[{value:"Introduction to Humanoid Navigation Challenges",id:"introduction-to-humanoid-navigation-challenges",level:2},{value:"Key Differences from Traditional Mobile Robot Navigation",id:"key-differences-from-traditional-mobile-robot-navigation",level:3},{value:"Simultaneous Localization and Mapping (SLAM)",id:"simultaneous-localization-and-mapping-slam",level:2},{value:"SLAM Fundamentals for Humanoid Robots",id:"slam-fundamentals-for-humanoid-robots",level:3},{value:"Types of SLAM for Humanoid Navigation",id:"types-of-slam-for-humanoid-navigation",level:3},{value:"Visual SLAM (vSLAM)",id:"visual-slam-vslam",level:4},{value:"LiDAR SLAM",id:"lidar-slam",level:4},{value:"Multi-Sensor Fusion SLAM",id:"multi-sensor-fusion-slam",level:4},{value:"ROS 2 Navigation Stack (Nav2)",id:"ros-2-navigation-stack-nav2",level:2},{value:"Overview of Navigation2",id:"overview-of-navigation2",level:3},{value:"Key Components of Nav2",id:"key-components-of-nav2",level:3},{value:"1. Navigation Server",id:"1-navigation-server",level:4},{value:"2. Path Planner",id:"2-path-planner",level:4},{value:"3. Path Follower",id:"3-path-follower",level:4},{value:"Humanoid-Specific Navigation Considerations",id:"humanoid-specific-navigation-considerations",level:2},{value:"Footstep Planning",id:"footstep-planning",level:3},{value:"Balance and Stability Considerations",id:"balance-and-stability-considerations",level:3},{value:"Navigation in Dynamic Environments",id:"navigation-in-dynamic-environments",level:2},{value:"Dynamic Obstacle Avoidance",id:"dynamic-obstacle-avoidance",level:3},{value:"Social Navigation",id:"social-navigation",level:3},{value:"Performance Optimization and Best Practices",id:"performance-optimization-and-best-practices",level:2},{value:"1. Multi-Resolution Mapping",id:"1-multi-resolution-mapping",level:3},{value:"2. Hierarchical Planning",id:"2-hierarchical-planning",level:3},{value:"3. Adaptive Navigation",id:"3-adaptive-navigation",level:3},{value:"Troubleshooting Common Navigation Issues",id:"troubleshooting-common-navigation-issues",level:2},{value:"1. Localization Drift",id:"1-localization-drift",level:3},{value:"2. Path Planning Failures",id:"2-path-planning-failures",level:3},{value:"3. Navigation Oscillation",id:"3-navigation-oscillation",level:3}];function p(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"humanoid-navigation-in-complex-environments",children:"Humanoid Navigation in Complex Environments"})}),"\n",(0,t.jsx)(e.h2,{id:"introduction-to-humanoid-navigation-challenges",children:"Introduction to Humanoid Navigation Challenges"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robot navigation presents unique challenges compared to wheeled or tracked robots due to their bipedal locomotion, complex kinematics, and anthropomorphic form factor. Unlike simpler mobile robots, humanoid robots must maintain balance while navigating, consider their multi-joint structure, and adapt their gait to traverse various terrains and obstacles."}),"\n",(0,t.jsx)(e.h3,{id:"key-differences-from-traditional-mobile-robot-navigation",children:"Key Differences from Traditional Mobile Robot Navigation"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Dynamic Balance"}),": Humanoid robots must maintain dynamic balance during movement"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Step Planning"}),": Requires discrete footstep planning rather than continuous path following"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Terrain Adaptation"}),": Must handle stairs, curbs, and uneven surfaces"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Upper Body Considerations"}),": Arm and torso movements affect balance and navigation"]}),"\n",(0,t.jsxs)(e.li,{children:[(0,t.jsx)(e.strong,{children:"Social Navigation"}),": Need to navigate around humans in shared spaces"]}),"\n"]}),"\n",(0,t.jsx)(e.h2,{id:"simultaneous-localization-and-mapping-slam",children:"Simultaneous Localization and Mapping (SLAM)"}),"\n",(0,t.jsx)(e.h3,{id:"slam-fundamentals-for-humanoid-robots",children:"SLAM Fundamentals for Humanoid Robots"}),"\n",(0,t.jsx)(e.p,{children:"SLAM is critical for humanoid robots operating in unknown or partially known environments. The process involves simultaneously building a map of the environment while tracking the robot's position within that map."}),"\n",(0,t.jsx)(e.h3,{id:"types-of-slam-for-humanoid-navigation",children:"Types of SLAM for Humanoid Navigation"}),"\n",(0,t.jsx)(e.h4,{id:"visual-slam-vslam",children:"Visual SLAM (vSLAM)"}),"\n",(0,t.jsx)(e.p,{children:"Visual SLAM uses camera data to estimate the robot's position and build maps:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"import numpy as np\nimport cv2\nfrom scipy.spatial.transform import Rotation as R\n\nclass VisualSLAM:\n    def __init__(self):\n        self.feature_detector = cv2.SIFT_create()\n        self.matcher = cv2.BFMatcher()\n        self.camera_matrix = np.array([[500, 0, 320],\n                                      [0, 500, 240],\n                                      [0, 0, 1]])\n        self.pose = np.eye(4)  # 4x4 transformation matrix\n        self.map_points = []   # 3D map points\n        self.keyframes = []    # Keyframe poses\n\n    def process_frame(self, image, timestamp):\n        # Detect features\n        keypoints, descriptors = self.feature_detector.detectAndCompute(image, None)\n\n        if len(self.keyframes) == 0:\n            # Initialize first frame\n            self.keyframes.append({\n                'pose': self.pose.copy(),\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'timestamp': timestamp\n            })\n            return self.pose\n\n        # Match with previous frame\n        prev_descriptors = self.keyframes[-1]['descriptors']\n        matches = self.matcher.knnMatch(descriptors, prev_descriptors, k=2)\n\n        # Apply ratio test\n        good_matches = []\n        for m, n in matches:\n            if m.distance < 0.75 * n.distance:\n                good_matches.append(m)\n\n        if len(good_matches) >= 10:\n            # Extract matched points\n            src_pts = np.float32([keypoints[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n            dst_pts = np.float32([self.keyframes[-1]['keypoints'][m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n\n            # Estimate essential matrix\n            E, mask = cv2.findEssentialMat(src_pts, dst_pts, self.camera_matrix,\n                                          method=cv2.RANSAC, threshold=1.0)\n\n            if E is not None:\n                # Recover pose\n                _, R, t, mask = cv2.recoverPose(E, src_pts, dst_pts, self.camera_matrix)\n\n                # Update pose\n                delta_pose = np.eye(4)\n                delta_pose[:3, :3] = R\n                delta_pose[:3, 3] = t.flatten()\n\n                self.pose = self.pose @ np.linalg.inv(delta_pose)\n\n        # Add current frame as keyframe if significant movement\n        if np.linalg.norm(self.pose[:3, 3] - self.keyframes[-1]['pose'][:3, 3]) > 0.5:\n            self.keyframes.append({\n                'pose': self.pose.copy(),\n                'keypoints': keypoints,\n                'descriptors': descriptors,\n                'timestamp': timestamp\n            })\n\n        return self.pose\n"})}),"\n",(0,t.jsx)(e.h4,{id:"lidar-slam",children:"LiDAR SLAM"}),"\n",(0,t.jsx)(e.p,{children:"LiDAR SLAM provides accurate distance measurements for robust mapping:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'import numpy as np\nfrom scipy.spatial.distance import cdist\n\nclass LiDARSLAM:\n    def __init__(self, map_resolution=0.1):\n        self.map_resolution = map_resolution\n        self.occupancy_map = {}  # Dictionary for sparse map\n        self.pose = np.zeros(3)  # x, y, theta\n        self.scan_history = []\n\n    def update_map(self, scan_points, robot_pose):\n        """\n        Update occupancy map with new LiDAR scan\n        """\n        # Convert scan points to global coordinates\n        global_points = self.transform_scan_to_global(scan_points, robot_pose)\n\n        # Update occupancy probabilities\n        for point in global_points:\n            grid_x, grid_y = self.world_to_grid(point[0], point[1])\n            grid_key = (grid_x, grid_y)\n\n            if grid_key in self.occupancy_map:\n                self.occupancy_map[grid_key] = self.update_occupancy(\n                    self.occupancy_map[grid_key], True\n                )\n            else:\n                self.occupancy_map[grid_key] = 0.7  # Initialize as occupied\n\n    def transform_scan_to_global(self, scan_points, robot_pose):\n        """\n        Transform LiDAR scan from robot frame to global frame\n        """\n        x_r, y_r, theta_r = robot_pose\n\n        # Rotation matrix\n        R = np.array([[np.cos(theta_r), -np.sin(theta_r)],\n                      [np.sin(theta_r), np.cos(theta_r)]])\n\n        # Transform points\n        local_points = np.array(scan_points)\n        global_points = np.dot(local_points, R.T) + np.array([x_r, y_r])\n\n        return global_points\n\n    def world_to_grid(self, x, y):\n        """\n        Convert world coordinates to grid coordinates\n        """\n        grid_x = int(np.round(x / self.map_resolution))\n        grid_y = int(np.round(y / self.map_resolution))\n        return grid_x, grid_y\n\n    def update_occupancy(self, current_prob, is_occupied):\n        """\n        Update occupancy probability using log-odds\n        """\n        log_odds = np.log(current_prob / (1 - current_prob))\n        if is_occupied:\n            log_odds += 0.9  # Increase occupancy\n        else:\n            log_odds -= 0.3  # Decrease occupancy\n        new_prob = 1 - 1 / (1 + np.exp(log_odds))\n        return np.clip(new_prob, 0.05, 0.95)\n'})}),"\n",(0,t.jsx)(e.h4,{id:"multi-sensor-fusion-slam",children:"Multi-Sensor Fusion SLAM"}),"\n",(0,t.jsx)(e.p,{children:"Combining multiple sensors for robust SLAM:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class MultiSensorSLAM:\n    def __init__(self):\n        self.visual_slam = VisualSLAM()\n        self.lidar_slam = LiDARSLAM()\n        self.imu_filter = IMUKalmanFilter()\n        self.fusion_weights = {\n            \'visual\': 0.4,\n            \'lidar\': 0.4,\n            \'imu\': 0.2\n        }\n\n    def process_sensors(self, camera_image, lidar_scan, imu_data, timestamp):\n        """\n        Process data from multiple sensors and fuse results\n        """\n        # Process individual sensors\n        visual_pose = self.visual_slam.process_frame(camera_image, timestamp)\n        lidar_pose = self.lidar_slam.process_scan(lidar_scan, timestamp)\n        imu_pose = self.imu_filter.update(imu_data, timestamp)\n\n        # Fuse poses using weighted average\n        fused_pose = self.fuse_poses(visual_pose, lidar_pose, imu_pose)\n\n        return fused_pose\n\n    def fuse_poses(self, visual_pose, lidar_pose, imu_pose):\n        """\n        Fuse poses from different sensors using weighted average\n        """\n        # Convert to common format and apply weights\n        # Implementation depends on specific pose representations\n        pass\n'})}),"\n",(0,t.jsx)(e.h2,{id:"ros-2-navigation-stack-nav2",children:"ROS 2 Navigation Stack (Nav2)"}),"\n",(0,t.jsx)(e.h3,{id:"overview-of-navigation2",children:"Overview of Navigation2"}),"\n",(0,t.jsx)(e.p,{children:"Navigation2 is the latest navigation stack for ROS 2, designed to provide robust and flexible navigation capabilities for mobile robots, including humanoid robots with appropriate modifications."}),"\n",(0,t.jsx)(e.h3,{id:"key-components-of-nav2",children:"Key Components of Nav2"}),"\n",(0,t.jsx)(e.h4,{id:"1-navigation-server",children:"1. Navigation Server"}),"\n",(0,t.jsx)(e.p,{children:"The Navigation Server orchestrates the navigation process:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"from rclpy.node import Node\nfrom nav2_msgs.action import NavigateToPose\nfrom rclpy.action import ActionServer\nimport tf2_ros\n\nclass HumanoidNavigationServer(Node):\n    def __init__(self):\n        super().__init__('humanoid_navigation_server')\n\n        # Action server for navigation\n        self._action_server = ActionServer(\n            self,\n            NavigateToPose,\n            'navigate_to_pose',\n            self.execute_navigate_to_pose\n        )\n\n        # TF buffer for coordinate transformations\n        self.tf_buffer = tf2_ros.Buffer()\n        self.tf_listener = tf2_ros.TransformListener(self.tf_buffer, self)\n\n        # Navigation components\n        self.map_client = self.create_client(GetMap, 'map_server/map')\n        self.planner = HumanoidPathPlanner()\n        self.controller = HumanoidPathFollower()\n        self.recovery = RecoveryNode()\n\n    def execute_navigate_to_pose(self, goal_handle):\n        \"\"\"\n        Execute navigation to specified pose\n        \"\"\"\n        goal_pose = goal_handle.request.pose\n\n        # Get current robot pose\n        current_pose = self.get_current_pose()\n\n        # Plan path considering humanoid constraints\n        path = self.planner.plan_path(current_pose, goal_pose)\n\n        if not path:\n            goal_handle.abort()\n            return NavigateToPose.Result()\n\n        # Execute path following\n        result = self.controller.follow_path(path, goal_pose)\n\n        if result.success:\n            goal_handle.succeed()\n        else:\n            goal_handle.abort()\n\n        return result\n"})}),"\n",(0,t.jsx)(e.h4,{id:"2-path-planner",children:"2. Path Planner"}),"\n",(0,t.jsx)(e.p,{children:"The path planner generates collision-free paths:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from nav2_core import GlobalPlanner\nfrom nav2_msgs.msg import Costmap\nimport numpy as np\n\nclass HumanoidPathPlanner(GlobalPlanner):\n    def __init__(self):\n        super().__init__(\'humanoid_path_planner\')\n        self.costmap = None\n        self.planner = AStarPlanner()  # or Dijkstra, RRT, etc.\n\n    def create_plan(self, start, goal, planner_id=\'\'):\n        """\n        Create plan from start to goal considering humanoid constraints\n        """\n        # Get updated costmap\n        costmap = self.get_costmap()\n\n        # Convert poses to grid coordinates\n        start_grid = self.pose_to_grid(start.pose)\n        goal_grid = self.pose_to_grid(goal.pose)\n\n        # Plan path with humanoid-specific constraints\n        path = self.planner.plan(\n            costmap=costmap,\n            start=start_grid,\n            goal=goal_grid,\n            robot_radius=self.get_humanoid_radius()\n        )\n\n        # Convert path back to poses\n        pose_path = self.path_to_poses(path)\n\n        return pose_path\n\n    def get_humanoid_radius(self):\n        """\n        Return effective radius considering humanoid form factor\n        """\n        # Account for robot\'s width and safety margin\n        return 0.4  # meters\n'})}),"\n",(0,t.jsx)(e.h4,{id:"3-path-follower",children:"3. Path Follower"}),"\n",(0,t.jsx)(e.p,{children:"The path follower executes the planned path:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'from nav2_core import LocalPlanner\nfrom geometry_msgs.msg import Twist\nfrom tf2_ros import TransformException\n\nclass HumanoidPathFollower(LocalPlanner):\n    def __init__(self):\n        super().__init__(\'humanoid_path_follower\')\n        self.cmd_vel_pub = None\n        self.current_path = None\n        self.path_index = 0\n        self.controller = PIDController()\n\n    def compute_velocity_commands(self, pose, velocity, goal_checker):\n        """\n        Compute velocity commands to follow the path\n        """\n        if not self.current_path or self.path_index >= len(self.current_path):\n            # Path completed\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.0\n            return cmd_vel, []\n\n        # Get next waypoint\n        target_pose = self.current_path[self.path_index]\n\n        # Calculate distance to target\n        dx = target_pose.pose.position.x - pose.pose.position.x\n        dy = target_pose.pose.position.y - pose.pose.position.y\n        distance = np.sqrt(dx**2 + dy**2)\n\n        # Check if reached current waypoint\n        if distance < 0.2:  # Waypoint tolerance\n            self.path_index += 1\n            if self.path_index >= len(self.current_path):\n                # Path completed\n                cmd_vel = Twist()\n                return cmd_vel, []\n\n        # Calculate desired velocity using humanoid-specific controller\n        cmd_vel = self.calculate_humanoid_velocity(pose, target_pose)\n\n        return cmd_vel, []\n\n    def calculate_humanoid_velocity(self, current_pose, target_pose):\n        """\n        Calculate velocity commands considering humanoid dynamics\n        """\n        # Calculate heading to target\n        dx = target_pose.pose.position.x - current_pose.pose.position.x\n        dy = target_pose.pose.position.y - current_pose.pose.position.y\n        target_heading = np.arctan2(dy, dx)\n\n        # Get current heading\n        current_heading = self.quaternion_to_yaw(current_pose.pose.orientation)\n\n        # Calculate heading error\n        heading_error = self.normalize_angle(target_heading - current_heading)\n\n        # Humanoid-specific velocity calculation\n        linear_vel = min(0.3, np.sqrt(dx**2 + dy**2) * 0.5)  # Max 0.3 m/s\n        angular_vel = heading_error * 1.0  # P controller\n\n        cmd_vel = Twist()\n        cmd_vel.linear.x = linear_vel\n        cmd_vel.angular.z = angular_vel\n\n        return cmd_vel\n'})}),"\n",(0,t.jsx)(e.h2,{id:"humanoid-specific-navigation-considerations",children:"Humanoid-Specific Navigation Considerations"}),"\n",(0,t.jsx)(e.h3,{id:"footstep-planning",children:"Footstep Planning"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots require discrete footstep planning rather than continuous path following:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class FootstepPlanner:\n    def __init__(self):\n        self.support_polygon = self.calculate_support_polygon()\n        self.step_constraints = {\n            \'max_step_width\': 0.3,   # meters\n            \'max_step_length\': 0.5,  # meters\n            \'min_step_height\': 0.05  # meters\n        }\n\n    def plan_footsteps(self, path, start_pose):\n        """\n        Plan discrete footsteps along the path\n        """\n        footsteps = []\n        current_pose = start_pose\n\n        for i in range(len(path)):\n            target_pose = path[i]\n\n            # Calculate step parameters\n            dx = target_pose.pose.position.x - current_pose.pose.position.x\n            dy = target_pose.pose.position.y - current_pose.pose.position.y\n            step_distance = np.sqrt(dx**2 + dy**2)\n\n            if step_distance > self.step_constraints[\'max_step_length\']:\n                # Interpolate additional steps\n                num_steps = int(np.ceil(step_distance / self.step_constraints[\'max_step_length\']))\n                for j in range(1, num_steps + 1):\n                    interp_pose = self.interpolate_pose(\n                        current_pose, target_pose, j / num_steps\n                    )\n                    footsteps.append(interp_pose)\n            else:\n                footsteps.append(target_pose)\n\n            current_pose = target_pose\n\n        return footsteps\n\n    def interpolate_pose(self, start_pose, end_pose, ratio):\n        """\n        Interpolate between two poses\n        """\n        interp_pose = PoseStamped()\n        interp_pose.pose.position.x = start_pose.pose.position.x + \\\n            (end_pose.pose.position.x - start_pose.pose.position.x) * ratio\n        interp_pose.pose.position.y = start_pose.pose.position.y + \\\n            (end_pose.pose.position.y - start_pose.pose.position.y) * ratio\n\n        # Interpolate orientation (simplified)\n        start_yaw = self.quaternion_to_yaw(start_pose.pose.orientation)\n        end_yaw = self.quaternion_to_yaw(end_pose.pose.orientation)\n        interp_yaw = start_yaw + (end_yaw - start_yaw) * ratio\n        interp_pose.pose.orientation = self.yaw_to_quaternion(interp_yaw)\n\n        return interp_pose\n'})}),"\n",(0,t.jsx)(e.h3,{id:"balance-and-stability-considerations",children:"Balance and Stability Considerations"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots must maintain balance during navigation:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class BalanceController:\n    def __init__(self):\n        self.zmp_reference = np.array([0.0, 0.0])  # Zero Moment Point reference\n        self.com_reference = np.array([0.0, 0.0, 0.8])  # Center of Mass reference\n        self.balance_gains = {\n            \'kp\': 10.0,\n            \'kd\': 2.0\n        }\n\n    def calculate_balance_control(self, current_state):\n        """\n        Calculate balance control corrections\n        """\n        # Get current ZMP and CoM\n        current_zmp = self.calculate_zmp(current_state)\n        current_com = self.calculate_com(current_state)\n\n        # Calculate errors\n        zmp_error = self.zmp_reference - current_zmp\n        com_error = self.com_reference - current_com\n\n        # Calculate balance corrections\n        zmp_correction = self.balance_gains[\'kp\'] * zmp_error + \\\n                        self.balance_gains[\'kd\'] * self.calculate_zmp_derivative(current_state)\n\n        return zmp_correction\n\n    def calculate_zmp(self, state):\n        """\n        Calculate Zero Moment Point from robot state\n        """\n        # Implementation depends on robot kinematics\n        pass\n'})}),"\n",(0,t.jsx)(e.h2,{id:"navigation-in-dynamic-environments",children:"Navigation in Dynamic Environments"}),"\n",(0,t.jsx)(e.h3,{id:"dynamic-obstacle-avoidance",children:"Dynamic Obstacle Avoidance"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots must navigate around moving obstacles and humans:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class DynamicObstacleAvoider:\n    def __init__(self):\n        self.obstacle_predictor = KalmanFilter()\n        self.trajectory_planner = TrajectoryOptimization()\n\n    def avoid_dynamic_obstacles(self, current_path, dynamic_obstacles):\n        """\n        Modify path to avoid moving obstacles\n        """\n        # Predict obstacle trajectories\n        predicted_obstacles = []\n        for obstacle in dynamic_obstacles:\n            predicted_trajectory = self.predict_obstacle_path(obstacle)\n            predicted_obstacles.append(predicted_trajectory)\n\n        # Plan collision-free trajectory\n        safe_trajectory = self.trajectory_planner.plan_with_obstacles(\n            current_path, predicted_obstacles\n        )\n\n        return safe_trajectory\n\n    def predict_obstacle_path(self, obstacle):\n        """\n        Predict future path of dynamic obstacle\n        """\n        # Use Kalman filter to predict obstacle motion\n        predicted_path = self.obstacle_predictor.predict(\n            obstacle.position, obstacle.velocity\n        )\n        return predicted_path\n'})}),"\n",(0,t.jsx)(e.h3,{id:"social-navigation",children:"Social Navigation"}),"\n",(0,t.jsx)(e.p,{children:"Humanoid robots should navigate considerately around humans:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class SocialNavigation:\n    def __init__(self):\n        self.social_force_model = SocialForceModel()\n        self.personal_space = 0.8  # meters\n\n    def navigate_with_social_awareness(self, target_pose, humans):\n        """\n        Navigate considering social interactions with humans\n        """\n        # Calculate social forces\n        social_forces = []\n        for human in humans:\n            force = self.calculate_social_force(target_pose, human)\n            social_forces.append(force)\n\n        # Modify navigation behavior based on social forces\n        modified_target = self.adjust_target_for_social_forces(\n            target_pose, social_forces\n        )\n\n        return modified_target\n\n    def calculate_social_force(self, robot_pose, human_pose):\n        """\n        Calculate social force from human on robot\n        """\n        distance = self.calculate_distance(robot_pose, human_pose)\n        if distance < self.personal_space:\n            # Repulsive force\n            direction = robot_pose - human_pose\n            force_magnitude = max(0, (self.personal_space - distance) * 5.0)\n            force = (direction / np.linalg.norm(direction)) * force_magnitude\n            return force\n        else:\n            return np.array([0.0, 0.0])\n'})}),"\n",(0,t.jsx)(e.h2,{id:"performance-optimization-and-best-practices",children:"Performance Optimization and Best Practices"}),"\n",(0,t.jsx)(e.h3,{id:"1-multi-resolution-mapping",children:"1. Multi-Resolution Mapping"}),"\n",(0,t.jsx)(e.p,{children:"Use different map resolutions for different navigation tasks:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class MultiResolutionMapper:\n    def __init__(self):\n        self.global_map = OccupancyGrid(resolution=0.1)  # Global planning\n        self.local_map = OccupancyGrid(resolution=0.05)  # Local navigation\n        self.footstep_map = OccupancyGrid(resolution=0.02)  # Footstep planning\n\n    def update_maps(self, sensor_data):\n        """\n        Update maps at different resolutions\n        """\n        # Update all maps with sensor data\n        self.global_map.update(sensor_data, filter_size=3)\n        self.local_map.update(sensor_data, filter_size=1)\n        self.footstep_map.update(sensor_data, filter_size=0.5)\n'})}),"\n",(0,t.jsx)(e.h3,{id:"2-hierarchical-planning",children:"2. Hierarchical Planning"}),"\n",(0,t.jsx)(e.p,{children:"Implement hierarchical planning for complex navigation tasks:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:'class HierarchicalNavigator:\n    def __init__(self):\n        self.global_planner = GlobalPlanner()\n        self.local_planner = LocalPlanner()\n        self.footstep_planner = FootstepPlanner()\n\n    def navigate(self, start, goal):\n        """\n        Navigate using hierarchical planning approach\n        """\n        # Global path planning\n        global_path = self.global_planner.plan(start, goal)\n\n        # Local path refinement\n        local_path = self.local_planner.refine(global_path)\n\n        # Footstep planning\n        footsteps = self.footstep_planner.plan_footsteps(local_path, start)\n\n        return footsteps\n'})}),"\n",(0,t.jsx)(e.h3,{id:"3-adaptive-navigation",children:"3. Adaptive Navigation"}),"\n",(0,t.jsx)(e.p,{children:"Adjust navigation parameters based on environment and robot state:"}),"\n",(0,t.jsx)(e.pre,{children:(0,t.jsx)(e.code,{className:"language-python",children:"class AdaptiveNavigator:\n    def __init__(self):\n        self.navigation_params = {\n            'linear_vel': 0.3,\n            'angular_vel': 0.5,\n            'inflation_radius': 0.5,\n            'min_distance': 0.3\n        }\n\n    def adapt_to_environment(self, environment_type):\n        \"\"\"\n        Adapt navigation parameters based on environment\n        \"\"\"\n        if environment_type == 'crowded':\n            self.navigation_params.update({\n                'linear_vel': 0.15,\n                'inflation_radius': 0.7,\n                'min_distance': 0.5\n            })\n        elif environment_type == 'open':\n            self.navigation_params.update({\n                'linear_vel': 0.4,\n                'inflation_radius': 0.3,\n                'min_distance': 0.2\n            })\n        elif environment_type == 'narrow':\n            self.navigation_params.update({\n                'linear_vel': 0.2,\n                'angular_vel': 0.3,\n                'inflation_radius': 0.4\n            })\n"})}),"\n",(0,t.jsx)(e.h2,{id:"troubleshooting-common-navigation-issues",children:"Troubleshooting Common Navigation Issues"}),"\n",(0,t.jsx)(e.h3,{id:"1-localization-drift",children:"1. Localization Drift"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Ensure sufficient visual features in the environment"}),"\n",(0,t.jsx)(e.li,{children:"Use multiple sensor fusion for robust localization"}),"\n",(0,t.jsx)(e.li,{children:"Implement loop closure detection"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"2-path-planning-failures",children:"2. Path Planning Failures"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Verify costmap inflation parameters"}),"\n",(0,t.jsx)(e.li,{children:"Check for proper map resolution"}),"\n",(0,t.jsx)(e.li,{children:"Ensure adequate obstacle detection"}),"\n"]}),"\n",(0,t.jsx)(e.h3,{id:"3-navigation-oscillation",children:"3. Navigation Oscillation"}),"\n",(0,t.jsxs)(e.ul,{children:["\n",(0,t.jsx)(e.li,{children:"Adjust controller gains appropriately"}),"\n",(0,t.jsx)(e.li,{children:"Increase waypoint tolerance if needed"}),"\n",(0,t.jsx)(e.li,{children:"Verify sensor data quality"}),"\n"]}),"\n",(0,t.jsx)(e.p,{children:"Humanoid navigation in complex environments requires sophisticated integration of SLAM, path planning, and balance control systems, with careful consideration of the unique challenges posed by bipedal locomotion and anthropomorphic form factors."})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(p,{...n})}):p(n)}},8453:(n,e,a)=>{a.d(e,{R:()=>s,x:()=>r});var o=a(6540);const t={},i=o.createContext(t);function s(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:s(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);