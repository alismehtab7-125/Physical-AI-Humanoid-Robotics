"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[795],{2822:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>r,default:()=>u,frontMatter:()=>l,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"module-04/module-04-intro","title":"Module 4 - Vision-Language-Action (V-L-A)","description":"Welcome to the fourth module of the Physical AI & Humanoid Robotics course. This module explores Vision-Language-Action (VLA) systems, which enable robots to understand and respond to complex human instructions through multimodal AI.","source":"@site/docs/module-04/index.md","sourceDirName":"module-04","slug":"/module-04/","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/","draft":false,"unlisted":false,"editUrl":"https://github.com/alismehtab7-125/Physical-AI-Humanoid-Robotics/edit/main/docs/module-04/index.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"id":"module-04-intro","title":"Module 4 - Vision-Language-Action (V-L-A)","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Reinforcement Learning for Humanoid Control","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-03/rl-for-humanoid-control"},"next":{"title":"Vision-Language Models (VLM) for Task Planning","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/introduction-to-vlm-llm"}}');var t=i(4848),s=i(8453);const l={id:"module-04-intro",title:"Module 4 - Vision-Language-Action (V-L-A)",sidebar_position:4},r="Module 4: Vision-Language-Action (VLA)",a={},d=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Topics Covered",id:"topics-covered",level:2},{value:"Hands-On Exercises",id:"hands-on-exercises",level:2}];function c(e){const n={h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(n.p,{children:"Welcome to the fourth module of the Physical AI & Humanoid Robotics course. This module explores Vision-Language-Action (VLA) systems, which enable robots to understand and respond to complex human instructions through multimodal AI."}),"\n",(0,t.jsx)(n.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this module, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Understand Vision-Language-Action (VLA) system architectures"}),"\n",(0,t.jsx)(n.li,{children:"Implement multimodal AI systems for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Integrate vision, language, and action components"}),"\n",(0,t.jsx)(n.li,{children:"Create human-robot interaction systems with natural language understanding"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Completion of Modules 1-3"}),"\n",(0,t.jsx)(n.li,{children:"Understanding of AI and computer vision concepts"}),"\n",(0,t.jsx)(n.li,{children:"Familiarity with natural language processing fundamentals"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"topics-covered",children:"Topics Covered"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Vision-Language-Action system architecture"}),"\n",(0,t.jsx)(n.li,{children:"Multimodal AI integration"}),"\n",(0,t.jsx)(n.li,{children:"Natural language understanding for robotics"}),"\n",(0,t.jsx)(n.li,{children:"Human-robot interaction design"}),"\n",(0,t.jsx)(n.li,{children:"Embodied AI systems"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"hands-on-exercises",children:"Hands-On Exercises"}),"\n",(0,t.jsx)(n.p,{children:"Throughout this module, you will create multimodal AI systems that enable robots to respond to human instructions."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>r});var o=i(6540);const t={},s=o.createContext(t);function l(e){const n=o.useContext(s);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:l(e.components),o.createElement(s.Provider,{value:n},e.children)}}}]);