"use strict";(globalThis.webpackChunkmy_website=globalThis.webpackChunkmy_website||[]).push([[696],{7109:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-04/capstone-project","title":"Capstone Project: Autonomous Humanoid Agent","description":"Project Overview","source":"@site/docs/module-04/20-capstone-project.md","sourceDirName":"module-04","slug":"/module-04/capstone-project","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/capstone-project","draft":false,"unlisted":false,"editUrl":"https://github.com/alismehtab7-125/Physical-AI-Humanoid-Robotics/edit/main/docs/module-04/20-capstone-project.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"Capstone Project: Autonomous Humanoid Agent"},"sidebar":"tutorialSidebar","previous":{"title":"Real-World Deployment and Challenges","permalink":"/Physical-AI-Humanoid-Robotics/docs/module-04/real-world-deployment"}}');var i=t(4848),o=t(8453);const a={sidebar_position:5,title:"Capstone Project: Autonomous Humanoid Agent"},r="Capstone Project: Autonomous Humanoid Agent",l={},c=[{value:"Project Overview",id:"project-overview",level:2},{value:"Project Objectives",id:"project-objectives",level:3},{value:"Project Scope",id:"project-scope",level:3},{value:"Project Architecture",id:"project-architecture",level:2},{value:"High-Level System Architecture",id:"high-level-system-architecture",level:3},{value:"ROS 2 Package Structure",id:"ros-2-package-structure",level:3},{value:"Detailed Implementation Requirements",id:"detailed-implementation-requirements",level:2},{value:"1. Natural Language Understanding System",id:"1-natural-language-understanding-system",level:3},{value:"LLM Integration Node",id:"llm-integration-node",level:4},{value:"Vision-Language Perception Node",id:"vision-language-perception-node",level:4},{value:"2. Navigation System",id:"2-navigation-system",level:3},{value:"Enhanced Navigation Node",id:"enhanced-navigation-node",level:4},{value:"3. Manipulation System",id:"3-manipulation-system",level:3},{value:"Grasping and Manipulation Node",id:"grasping-and-manipulation-node",level:4},{value:"4. Simulation Integration",id:"4-simulation-integration",level:3},{value:"Simulation Bridge Node",id:"simulation-bridge-node",level:4},{value:"Project Milestones and Deliverables",id:"project-milestones-and-deliverables",level:2},{value:"Milestone 1: System Integration (Weeks 1-2)",id:"milestone-1-system-integration-weeks-1-2",level:3},{value:"Milestone 2: Perception System (Weeks 3-4)",id:"milestone-2-perception-system-weeks-3-4",level:3},{value:"Milestone 3: Task Planning (Weeks 5-6)",id:"milestone-3-task-planning-weeks-5-6",level:3},{value:"Milestone 4: Manipulation (Weeks 7-8)",id:"milestone-4-manipulation-weeks-7-8",level:3},{value:"Milestone 5: Full Integration and Testing (Weeks 9-10)",id:"milestone-5-full-integration-and-testing-weeks-9-10",level:3},{value:"Evaluation Criteria",id:"evaluation-criteria",level:2},{value:"Technical Requirements (70%)",id:"technical-requirements-70",level:3},{value:"Performance Metrics (20%)",id:"performance-metrics-20",level:3},{value:"Documentation and Presentation (10%)",id:"documentation-and-presentation-10",level:3},{value:"Safety Considerations",id:"safety-considerations",level:2},{value:"Physical Safety",id:"physical-safety",level:3},{value:"Data Safety",id:"data-safety",level:3},{value:"Operational Safety",id:"operational-safety",level:3},{value:"Resources and References",id:"resources-and-references",level:2},{value:"Required Software",id:"required-software",level:3},{value:"Recommended Hardware (for physical deployment)",id:"recommended-hardware-for-physical-deployment",level:3}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"capstone-project-autonomous-humanoid-agent",children:"Capstone Project: Autonomous Humanoid Agent"})}),"\n",(0,i.jsx)(n.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,i.jsx)(n.p,{children:"The Capstone Project represents the culmination of the Physical AI & Humanoid Robotics course, integrating all concepts learned across the four modules. Students will develop a fully autonomous humanoid agent capable of understanding natural language commands, navigating complex environments, and executing sophisticated manipulation tasks using a combination of ROS 2, simulation environments (Gazebo/Unity), and AI planning systems (LLM/VLM)."}),"\n",(0,i.jsx)(n.h3,{id:"project-objectives",children:"Project Objectives"}),"\n",(0,i.jsx)(n.p,{children:"By completing this capstone project, students will demonstrate mastery of:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration"}),": Implementing distributed robotic systems with proper communication patterns"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulation-to-Reality Transfer"}),": Developing and testing in simulation before deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AI Planning"}),": Using LLMs and VLMs for high-level task understanding and planning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception Systems"}),": Implementing multimodal perception for environment understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Control Systems"}),": Developing stable humanoid locomotion and manipulation controllers"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety and Reliability"}),": Implementing proper safety protocols and error handling"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"project-scope",children:"Project Scope"}),"\n",(0,i.jsx)(n.p,{children:"The autonomous humanoid agent should be capable of:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:'Understanding natural language commands (e.g., "Go to the kitchen and bring me the red cup")'}),"\n",(0,i.jsx)(n.li,{children:"Navigating to specified locations while avoiding obstacles"}),"\n",(0,i.jsx)(n.li,{children:"Identifying and manipulating objects in the environment"}),"\n",(0,i.jsx)(n.li,{children:"Handling unexpected situations and recovering from failures"}),"\n",(0,i.jsx)(n.li,{children:"Providing feedback on task execution status"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"project-architecture",children:"Project Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"high-level-system-architecture",children:"High-Level System Architecture"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-mermaid",children:"graph TB\n    A[User Command] --\x3e B[LLM Task Parser]\n    B --\x3e C[PDDL Planner]\n    C --\x3e D[Navigation System]\n    C --\x3e E[Manipulation System]\n    C --\x3e F[Task Monitor]\n\n    G[Perception System] --\x3e D\n    G[Perception System] --\x3e E\n    G --\x3e C\n\n    H[Simulation Environment] --\x3e G\n    H --\x3e D\n    H --\x3e E\n\n    I[Real Robot] --\x3e G\n    I --\x3e D\n    I --\x3e E\n\n    D --\x3e F\n    E --\x3e F\n    F --\x3e A\n"})}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-package-structure",children:"ROS 2 Package Structure"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"capstone_project/\n\u251c\u2500\u2500 capstone_bringup/           # Launch files and configurations\n\u251c\u2500\u2500 capstone_perception/        # Vision-language perception nodes\n\u251c\u2500\u2500 capstone_planning/          # Task planning and LLM integration\n\u251c\u2500\u2500 capstone_navigation/        # Navigation stack extensions\n\u251c\u2500\u2500 capstone_manipulation/      # Manipulation and grasping\n\u251c\u2500\u2500 capstone_simulation/        # Simulation interfaces\n\u251c\u2500\u2500 capstone_control/           # Low-level control\n\u2514\u2500\u2500 capstone_interfaces/        # Custom message/service definitions\n"})}),"\n",(0,i.jsx)(n.h2,{id:"detailed-implementation-requirements",children:"Detailed Implementation Requirements"}),"\n",(0,i.jsx)(n.h3,{id:"1-natural-language-understanding-system",children:"1. Natural Language Understanding System"}),"\n",(0,i.jsx)(n.h4,{id:"llm-integration-node",children:"LLM Integration Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# capstone_planning/llm_task_parser.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom capstone_interfaces.srv import ParseCommand\nfrom capstone_interfaces.msg import TaskPlan\nimport openai\nimport json\n\nclass LLMTaskParser(Node):\n    def __init__(self):\n        super().__init__(\'llm_task_parser\')\n\n        # Service for parsing commands\n        self.parse_service = self.create_service(\n            ParseCommand, \'parse_task_command\', self.parse_command_callback\n        )\n\n        # Publisher for task plans\n        self.plan_pub = self.create_publisher(TaskPlan, \'task_plan\', 10)\n\n        # World state subscriber\n        self.world_state_sub = self.create_subscription(\n            String, \'world_state\', self.world_state_callback, 10\n        )\n\n        self.world_state = {}\n        self.llm_client = openai.OpenAI(api_key=self.get_parameter(\'openai_api_key\', \'\'))\n        self.model = self.get_parameter(\'llm_model\', \'gpt-3.5-turbo\')\n\n    def parse_command_callback(self, request, response):\n        """\n        Parse natural language command into structured task plan\n        """\n        try:\n            # Get current world state\n            world_state = self.world_state\n\n            # Create LLM prompt\n            prompt = self.create_parsing_prompt(request.command, world_state)\n\n            # Call LLM\n            completion = self.llm_client.chat.completions.create(\n                model=self.model,\n                messages=[{"role": "user", "content": prompt}],\n                temperature=0.3\n            )\n\n            # Parse response\n            structured_response = json.loads(completion.choices[0].message.content)\n\n            # Create task plan\n            task_plan = self.create_task_plan(structured_response)\n\n            # Publish plan\n            self.plan_pub.publish(task_plan)\n\n            response.success = True\n            response.plan = task_plan\n            response.message = "Successfully parsed command"\n\n        except Exception as e:\n            response.success = False\n            response.message = f"Failed to parse command: {str(e)}"\n\n        return response\n\n    def create_parsing_prompt(self, command, world_state):\n        """\n        Create prompt for LLM to parse command\n        """\n        return f"""\n        Parse the following natural language command into a structured task plan:\n\n        Command: "{command}"\n\n        Current World State: {json.dumps(world_state, indent=2)}\n\n        Provide a detailed task plan with the following structure:\n        {{\n            "tasks": [\n                {{\n                    "action": "action_type",\n                    "parameters": {{"param1": "value1", ...}},\n                    "description": "Human-readable description"\n                }}\n            ],\n            "dependencies": [...],\n            "success_criteria": [...]\n        }}\n\n        Action types: navigate, locate_object, grasp_object, place_object, speak, wait\n        """\n\n    def create_task_plan(self, structured_response):\n        """\n        Create TaskPlan message from structured response\n        """\n        plan_msg = TaskPlan()\n        plan_msg.header.stamp = self.get_clock().now().to_msg()\n        plan_msg.header.frame_id = "map"\n\n        for task in structured_response.get("tasks", []):\n            task_msg = TaskPlan.Task()\n            task_msg.action = task["action"]\n            task_msg.parameters = json.dumps(task.get("parameters", {}))\n            task_msg.description = task.get("description", "")\n            plan_msg.tasks.append(task_msg)\n\n        return plan_msg\n\n    def world_state_callback(self, msg):\n        """\n        Update world state from message\n        """\n        try:\n            self.world_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().warn("Failed to parse world state message")\n'})}),"\n",(0,i.jsx)(n.h4,{id:"vision-language-perception-node",children:"Vision-Language Perception Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# capstone_perception/vlm_perceptor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom capstone_interfaces.msg import ObjectDetection\nimport cv2\nfrom cv2 import aruco\nimport numpy as np\nimport torch\nfrom transformers import CLIPProcessor, CLIPModel\n\nclass VLMPerceptor(Node):\n    def __init__(self):\n        super().__init__(\'vlm_perceptor\')\n\n        # Initialize CLIP model\n        self.clip_model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")\n        self.clip_processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")\n\n        # Image subscriber\n        self.image_sub = self.create_subscription(\n            Image, \'camera/image_raw\', self.image_callback, 10\n        )\n\n        # Object detection publisher\n        self.detection_pub = self.create_publisher(ObjectDetection, \'object_detections\', 10)\n\n        # World state publisher\n        self.world_state_pub = self.create_publisher(String, \'world_state\', 10)\n\n        # Object classes to detect\n        self.object_classes = [\n            "cup", "bottle", "book", "phone", "chair", "table",\n            "kitchen", "living room", "bedroom", "refrigerator"\n        ]\n\n    def image_callback(self, msg):\n        """\n        Process image and detect objects using VLM\n        """\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.ros_image_to_cv2(msg)\n\n            # Detect objects using CLIP\n            detections = self.detect_objects_with_clip(cv_image)\n\n            # Publish detections\n            detection_msg = ObjectDetection()\n            detection_msg.header = msg.header\n            detection_msg.objects = detections\n            self.detection_pub.publish(detection_msg)\n\n            # Update world state\n            world_state = self.update_world_state(detections, cv_image.shape)\n            state_msg = String()\n            state_msg.data = json.dumps(world_state)\n            self.world_state_pub.publish(state_msg)\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing image: {e}")\n\n    def detect_objects_with_clip(self, image):\n        """\n        Use CLIP to detect objects in image\n        """\n        # Prepare text descriptions\n        text_descriptions = [f"a photo of a {obj}" for obj in self.object_classes]\n\n        # Process image and text\n        inputs = self.clip_processor(images=image, text=text_descriptions, return_tensors="pt", padding=True)\n\n        with torch.no_grad():\n            outputs = self.clip_model(**inputs)\n\n        # Get similarity scores\n        logits_per_image = outputs.logits_per_image\n        probs = logits_per_image.softmax(dim=-1).cpu().numpy()[0]\n\n        # Create detections\n        detections = []\n        for i, (obj_class, prob) in enumerate(zip(self.object_classes, probs)):\n            if prob > 0.1:  # Threshold\n                detection = ObjectDetection.Detection()\n                detection.name = obj_class\n                detection.confidence = float(prob)\n                detection.x = 0.0  # To be implemented with actual detection coordinates\n                detection.y = 0.0\n                detection.z = 0.0\n                detections.append(detection)\n\n        return detections\n\n    def update_world_state(self, detections, image_shape):\n        """\n        Update world state based on detections\n        """\n        world_state = {\n            "timestamp": self.get_clock().now().nanoseconds,\n            "objects": {},\n            "robot": {\n                "location": "unknown",\n                "battery_level": 100.0\n            },\n            "environment": {\n                "resolution": image_shape[:2]\n            }\n        }\n\n        for detection in detections:\n            world_state["objects"][detection.name] = {\n                "confidence": detection.confidence,\n                "location": {"x": detection.x, "y": detection.y, "z": detection.z}\n            }\n\n        return world_state\n'})}),"\n",(0,i.jsx)(n.h3,{id:"2-navigation-system",children:"2. Navigation System"}),"\n",(0,i.jsx)(n.h4,{id:"enhanced-navigation-node",children:"Enhanced Navigation Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# capstone_navigation/humanoid_navigator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import PoseStamped, Twist\nfrom nav_msgs.msg import Path\nfrom sensor_msgs.msg import LaserScan, PointCloud2\nfrom capstone_interfaces.msg import TaskPlan\nfrom visualization_msgs.msg import MarkerArray\nimport numpy as np\nfrom scipy.spatial.transform import Rotation as R\n\nclass HumanoidNavigator(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_navigator\')\n\n        # Navigation state\n        self.current_pose = None\n        self.current_plan = None\n        self.plan_index = 0\n\n        # Publishers and subscribers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.path_pub = self.create_publisher(Path, \'current_path\', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, \'navigation_markers\', 10)\n\n        # Task plan subscriber\n        self.plan_sub = self.create_subscription(\n            TaskPlan, \'task_plan\', self.plan_callback, 10\n        )\n\n        # Sensor subscribers\n        self.laser_sub = self.create_subscription(\n            LaserScan, \'scan\', self.laser_callback, 10\n        )\n\n        self.odom_sub = self.create_subscription(\n            PoseStamped, \'odom\', self.odom_callback, 10\n        )\n\n        # Navigation parameters\n        self.linear_vel = 0.3  # m/s\n        self.angular_vel = 0.5  # rad/s\n        self.waypoint_tolerance = 0.3  # meters\n        self.rotation_tolerance = 0.1  # radians\n\n        # Timer for navigation control\n        self.nav_timer = self.create_timer(0.1, self.navigation_control)\n\n    def plan_callback(self, msg):\n        """\n        Receive task plan and extract navigation tasks\n        """\n        for task in msg.tasks:\n            if task.action == "navigate":\n                params = json.loads(task.parameters)\n                target_location = params.get("location", "unknown")\n\n                # Convert location to coordinates (this would use a location map)\n                target_pose = self.location_to_pose(target_location)\n\n                if target_pose:\n                    self.follow_path_to_pose(target_pose)\n\n    def follow_path_to_pose(self, target_pose):\n        """\n        Generate and follow path to target pose\n        """\n        # For now, create a simple path to target\n        current_pose = self.current_pose\n\n        if current_pose is None:\n            self.get_logger().warn("Current pose not available")\n            return\n\n        # Simple straight-line path (in a real system, this would use a proper planner)\n        path = self.generate_simple_path(current_pose, target_pose)\n\n        self.current_plan = path\n        self.plan_index = 0\n\n        # Publish path for visualization\n        path_msg = Path()\n        path_msg.header.stamp = self.get_clock().now().to_msg()\n        path_msg.header.frame_id = "map"\n\n        for pose in path:\n            stamped_pose = PoseStamped()\n            stamped_pose.header = path_msg.header\n            stamped_pose.pose = pose\n            path_msg.poses.append(stamped_pose)\n\n        self.path_pub.publish(path_msg)\n\n    def navigation_control(self):\n        """\n        Main navigation control loop\n        """\n        if self.current_plan is None or self.plan_index >= len(self.current_plan):\n            # No active plan, stop the robot\n            cmd_vel = Twist()\n            self.cmd_vel_pub.publish(cmd_vel)\n            return\n\n        target_pose = self.current_plan[self.plan_index].pose\n        current_pose = self.current_pose\n\n        if current_pose is None:\n            return\n\n        # Calculate distance to target waypoint\n        dx = target_pose.position.x - current_pose.position.x\n        dy = target_pose.position.y - current_pose.position.y\n        distance = np.sqrt(dx**2 + dy**2)\n\n        # Check if reached current waypoint\n        if distance < self.waypoint_tolerance:\n            self.plan_index += 1\n            if self.plan_index >= len(self.current_plan):\n                # Path completed\n                cmd_vel = Twist()\n                self.cmd_vel_pub.publish(cmd_vel)\n                self.get_logger().info("Navigation task completed")\n                return\n\n        # Calculate control commands\n        cmd_vel = Twist()\n\n        # Linear velocity towards target\n        if distance > self.waypoint_tolerance:\n            cmd_vel.linear.x = min(self.linear_vel, distance * 0.5)\n\n        # Angular velocity for heading correction\n        target_heading = np.arctan2(dy, dx)\n        current_heading = self.quaternion_to_yaw(current_pose.orientation)\n        heading_error = self.normalize_angle(target_heading - current_heading)\n\n        cmd_vel.angular.z = max(-self.angular_vel, min(self.angular_vel, heading_error * 2.0))\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd_vel)\n\n    def laser_callback(self, msg):\n        """\n        Handle laser scan data for obstacle avoidance\n        """\n        # Implement obstacle detection and avoidance\n        min_distance = min(msg.ranges)\n\n        if min_distance < 0.5:  # Obstacle too close\n            # Emergency stop or path replanning\n            cmd_vel = Twist()\n            cmd_vel.linear.x = 0.0\n            cmd_vel.angular.z = 0.5  # Try to rotate away\n            self.cmd_vel_pub.publish(cmd_vel)\n\n    def odom_callback(self, msg):\n        """\n        Update current pose from odometry\n        """\n        self.current_pose = msg.pose\n\n    def quaternion_to_yaw(self, quaternion):\n        """\n        Convert quaternion to yaw angle\n        """\n        q = [quaternion.w, quaternion.x, quaternion.y, quaternion.z]\n        r = R.from_quat([q[1], q[2], q[3], q[0]])  # [x, y, z, w] format\n        return r.as_euler(\'xyz\')[2]  # yaw is the z component\n\n    def normalize_angle(self, angle):\n        """\n        Normalize angle to [-pi, pi] range\n        """\n        while angle > np.pi:\n            angle -= 2 * np.pi\n        while angle < -np.pi:\n            angle += 2 * np.pi\n        return angle\n\n    def location_to_pose(self, location_name):\n        """\n        Convert location name to coordinates (simplified)\n        """\n        location_map = {\n            "kitchen": (5.0, 2.0, 0.0),\n            "living_room": (0.0, 0.0, 0.0),\n            "bedroom": (-3.0, 4.0, 0.0),\n            "office": (2.0, -3.0, 0.0)\n        }\n\n        if location_name in location_map:\n            x, y, theta = location_map[location_name]\n            pose = Pose()\n            pose.position.x = x\n            pose.position.y = y\n            pose.position.z = 0.0\n\n            # Convert theta to quaternion\n            from tf_transformations import quaternion_from_euler\n            quat = quaternion_from_euler(0, 0, theta)\n            pose.orientation.w = quat[0]\n            pose.orientation.x = quat[1]\n            pose.orientation.y = quat[2]\n            pose.orientation.z = quat[3]\n\n            return pose\n\n        return None\n'})}),"\n",(0,i.jsx)(n.h3,{id:"3-manipulation-system",children:"3. Manipulation System"}),"\n",(0,i.jsx)(n.h4,{id:"grasping-and-manipulation-node",children:"Grasping and Manipulation Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# capstone_manipulation/manipulator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point\nfrom capstone_interfaces.msg import ObjectDetection, TaskPlan\nfrom sensor_msgs.msg import JointState\nimport numpy as np\n\nclass Manipulator(Node):\n    def __init__(self):\n        super().__init__(\'manipulator\')\n\n        # Publishers and subscribers\n        self.joint_cmd_pub = self.create_publisher(JointState, \'joint_commands\', 10)\n        self.task_sub = self.create_subscription(TaskPlan, \'task_plan\', self.task_callback, 10)\n        self.detection_sub = self.create_subscription(ObjectDetection, \'object_detections\', self.detection_callback, 10)\n\n        # Current object detections\n        self.current_detections = []\n\n        # Robot kinematic parameters (simplified)\n        self.link_lengths = [0.3, 0.3, 0.2]  # Link lengths for simple arm model\n\n    def task_callback(self, msg):\n        """\n        Handle manipulation tasks from plan\n        """\n        for task in msg.tasks:\n            if task.action in ["grasp_object", "place_object"]:\n                params = json.loads(task.parameters)\n\n                if task.action == "grasp_object":\n                    self.grasp_object(params["object_name"])\n                elif task.action == "place_object":\n                    self.place_object(params["location"])\n\n    def detection_callback(self, msg):\n        """\n        Update object detections\n        """\n        self.current_detections = msg.objects\n\n    def grasp_object(self, object_name):\n        """\n        Execute grasping motion for specified object\n        """\n        # Find object in current detections\n        target_object = None\n        for obj in self.current_detections:\n            if obj.name == object_name:\n                target_object = obj\n                break\n\n        if target_object is None:\n            self.get_logger().warn(f"Object {object_name} not found")\n            return\n\n        # Calculate grasp pose\n        grasp_pose = self.calculate_grasp_pose(target_object)\n\n        # Execute reaching motion\n        self.move_to_pose(grasp_pose)\n\n        # Close gripper\n        self.close_gripper()\n\n        self.get_logger().info(f"Successfully grasped {object_name}")\n\n    def place_object(self, location):\n        """\n        Place currently held object at specified location\n        """\n        # Calculate placement pose based on location\n        place_pose = self.calculate_place_pose(location)\n\n        # Execute placement motion\n        self.move_to_pose(place_pose)\n\n        # Open gripper\n        self.open_gripper()\n\n        self.get_logger().info(f"Successfully placed object at {location}")\n\n    def calculate_grasp_pose(self, object_detection):\n        """\n        Calculate appropriate grasp pose for object\n        """\n        # For a cup, approach from above with gripper aligned\n        grasp_pose = Pose()\n        grasp_pose.position.x = object_detection.x\n        grasp_pose.position.y = object_detection.y\n        grasp_pose.position.z = object_detection.z + 0.1  # Approach from above\n\n        # Orientation: gripper facing down\n        grasp_pose.orientation.w = 1.0  # No rotation\n        grasp_pose.orientation.x = 0.0\n        grasp_pose.orientation.y = 0.0\n        grasp_pose.orientation.z = 0.0\n\n        return grasp_pose\n\n    def calculate_place_pose(self, location):\n        """\n        Calculate placement pose for location\n        """\n        # Simplified: place at location coordinates at table height\n        place_pose = Pose()\n\n        # Convert location to coordinates\n        location_coords = self.location_to_coordinates(location)\n        place_pose.position.x = location_coords[0]\n        place_pose.position.y = location_coords[1]\n        place_pose.position.z = 0.8  # Table height\n\n        # Same orientation as grasp\n        place_pose.orientation.w = 1.0\n        place_pose.orientation.x = 0.0\n        place_pose.orientation.y = 0.0\n        place_pose.orientation.z = 0.0\n\n        return place_pose\n\n    def move_to_pose(self, target_pose):\n        """\n        Move manipulator to target pose (simplified)\n        """\n        # In a real system, this would use inverse kinematics\n        # For simulation, we\'ll just publish joint angles\n        joint_state = JointState()\n        joint_state.name = [\'joint1\', \'joint2\', \'joint3\', \'gripper\']\n        joint_state.position = [0.0, 0.0, 0.0, 0.0]  # Placeholder values\n\n        self.joint_cmd_pub.publish(joint_state)\n\n    def close_gripper(self):\n        """\n        Close the gripper\n        """\n        joint_state = JointState()\n        joint_state.name = [\'gripper\']\n        joint_state.position = [0.01]  # Closed position\n        self.joint_cmd_pub.publish(joint_state)\n\n    def open_gripper(self):\n        """\n        Open the gripper\n        """\n        joint_state = JointState()\n        joint_state.name = [\'gripper\']\n        joint_state.position = [0.05]  # Open position\n        self.joint_cmd_pub.publish(joint_state)\n\n    def location_to_coordinates(self, location):\n        """\n        Convert location name to coordinates\n        """\n        location_map = {\n            "kitchen_counter": (4.5, 1.5),\n            "kitchen_table": (5.5, 2.5),\n            "living_room_table": (0.5, 0.5),\n            "bedroom_nightstand": (-2.5, 3.5)\n        }\n\n        return location_map.get(location, (0.0, 0.0))\n'})}),"\n",(0,i.jsx)(n.h3,{id:"4-simulation-integration",children:"4. Simulation Integration"}),"\n",(0,i.jsx)(n.h4,{id:"simulation-bridge-node",children:"Simulation Bridge Node"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# capstone_simulation/sim_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom sensor_msgs.msg import Image, LaserScan, JointState\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom capstone_interfaces.msg import ObjectDetection\nimport numpy as np\n\nclass SimulationBridge(Node):\n    def __init__(self):\n        super().__init__(\'simulation_bridge\')\n\n        # Publishers for simulation data\n        self.image_pub = self.create_publisher(Image, \'camera/image_raw\', 10)\n        self.laser_pub = self.create_publisher(LaserScan, \'scan\', 10)\n        self.joint_state_pub = self.create_publisher(JointState, \'joint_states\', 10)\n        self.odom_pub = self.create_publisher(PoseStamped, \'odom\', 10)\n\n        # Subscribers for commands\n        self.cmd_vel_sub = self.create_subscription(Twist, \'cmd_vel\', self.cmd_vel_callback, 10)\n        self.joint_cmd_sub = self.create_subscription(JointState, \'joint_commands\', self.joint_cmd_callback, 10)\n\n        # Simulation state\n        self.robot_pose = np.array([0.0, 0.0, 0.0])  # x, y, theta\n        self.joint_positions = np.zeros(19)  # Humanoid joint positions\n\n        # Timer for simulation updates\n        self.sim_timer = self.create_timer(0.01, self.simulation_step)  # 100 Hz\n\n    def simulation_step(self):\n        """\n        Main simulation update step\n        """\n        # Update robot pose based on current velocity commands\n        self.update_robot_dynamics()\n\n        # Publish sensor data\n        self.publish_camera_data()\n        self.publish_laser_data()\n        self.publish_joint_states()\n        self.publish_odometry()\n\n    def update_robot_dynamics(self):\n        """\n        Update robot dynamics based on commands\n        """\n        # Simplified dynamics model\n        dt = 0.01  # Time step\n\n        # Update pose based on velocity (simplified)\n        if hasattr(self, \'current_cmd_vel\'):\n            vx = self.current_cmd_vel.linear.x\n            wz = self.current_cmd_vel.angular.z\n\n            # Update position\n            self.robot_pose[0] += vx * np.cos(self.robot_pose[2]) * dt\n            self.robot_pose[1] += vx * np.sin(self.robot_pose[2]) * dt\n            self.robot_pose[2] += wz * dt\n\n    def publish_camera_data(self):\n        """\n        Publish simulated camera data\n        """\n        # In a real simulation, this would come from the graphics engine\n        # For this example, we\'ll publish empty data\n        img_msg = Image()\n        img_msg.header.stamp = self.get_clock().now().to_msg()\n        img_msg.header.frame_id = \'camera_frame\'\n        img_msg.height = 480\n        img_msg.width = 640\n        img_msg.encoding = \'rgb8\'\n        img_msg.step = 640 * 3  # Width * bytes per pixel\n        img_msg.data = [0] * (480 * 640 * 3)  # Empty image data\n\n        self.image_pub.publish(img_msg)\n\n    def publish_laser_data(self):\n        """\n        Publish simulated laser scan data\n        """\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = self.get_clock().now().to_msg()\n        scan_msg.header.frame_id = \'laser_frame\'\n        scan_msg.angle_min = -np.pi / 2\n        scan_msg.angle_max = np.pi / 2\n        scan_msg.angle_increment = np.pi / 180  # 1 degree\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 10.0\n\n        # Generate simulated ranges (simplified)\n        num_ranges = int((scan_msg.angle_max - scan_msg.angle_min) / scan_msg.angle_increment) + 1\n        scan_msg.ranges = [5.0] * num_ranges  # All ranges at 5m (no obstacles)\n\n        self.laser_pub.publish(scan_msg)\n\n    def publish_joint_states(self):\n        """\n        Publish joint state information\n        """\n        joint_state = JointState()\n        joint_state.header.stamp = self.get_clock().now().to_msg()\n        joint_state.name = [f\'joint_{i}\' for i in range(len(self.joint_positions))]\n        joint_state.position = self.joint_positions.tolist()\n\n        self.joint_state_pub.publish(joint_state)\n\n    def publish_odometry(self):\n        """\n        Publish odometry information\n        """\n        odom_msg = PoseStamped()\n        odom_msg.header.stamp = self.get_clock().now().to_msg()\n        odom_msg.header.frame_id = \'map\'\n        odom_msg.pose.position.x = float(self.robot_pose[0])\n        odom_msg.pose.position.y = float(self.robot_pose[1])\n        odom_msg.pose.position.z = 0.0\n\n        # Convert theta to quaternion\n        from tf_transformations import quaternion_from_euler\n        quat = quaternion_from_euler(0, 0, self.robot_pose[2])\n        odom_msg.pose.orientation.w = quat[0]\n        odom_msg.pose.orientation.x = quat[1]\n        odom_msg.pose.orientation.y = quat[2]\n        odom_msg.pose.orientation.z = quat[3]\n\n        self.odom_pub.publish(odom_msg)\n\n    def cmd_vel_callback(self, msg):\n        """\n        Handle velocity commands\n        """\n        self.current_cmd_vel = msg\n\n    def joint_cmd_callback(self, msg):\n        """\n        Handle joint commands\n        """\n        # Update joint positions based on commands\n        for i, name in enumerate(msg.name):\n            try:\n                idx = self.joint_positions.index(name)  # This is simplified\n                self.joint_positions[idx] = msg.position[i]\n            except ValueError:\n                # Joint name not found in our list\n                pass\n'})}),"\n",(0,i.jsx)(n.h2,{id:"project-milestones-and-deliverables",children:"Project Milestones and Deliverables"}),"\n",(0,i.jsx)(n.h3,{id:"milestone-1-system-integration-weeks-1-2",children:"Milestone 1: System Integration (Weeks 1-2)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deliverables:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Basic ROS 2 workspace with all required packages"}),"\n",(0,i.jsx)(n.li,{children:"Simulation environment setup (Gazebo/Unity)"}),"\n",(0,i.jsx)(n.li,{children:"Communication between nodes verified"}),"\n",(0,i.jsx)(n.li,{children:"Basic navigation functionality"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tasks:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Set up ROS 2 workspace and dependencies"}),"\n",(0,i.jsx)(n.li,{children:"Integrate simulation environment"}),"\n",(0,i.jsx)(n.li,{children:"Implement basic message passing"}),"\n",(0,i.jsx)(n.li,{children:"Test navigation in simulation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"milestone-2-perception-system-weeks-3-4",children:"Milestone 2: Perception System (Weeks 3-4)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deliverables:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"VLM-based object detection working"}),"\n",(0,i.jsx)(n.li,{children:"World state representation"}),"\n",(0,i.jsx)(n.li,{children:"Integration with navigation system"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tasks:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement CLIP-based object detection"}),"\n",(0,i.jsx)(n.li,{children:"Create world state publisher"}),"\n",(0,i.jsx)(n.li,{children:"Integrate perception with navigation"}),"\n",(0,i.jsx)(n.li,{children:"Test object detection in simulation"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"milestone-3-task-planning-weeks-5-6",children:"Milestone 3: Task Planning (Weeks 5-6)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deliverables:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"LLM-based command parsing"}),"\n",(0,i.jsx)(n.li,{children:"PDDL task planner integration"}),"\n",(0,i.jsx)(n.li,{children:"Task execution monitoring"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tasks:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate LLM for command parsing"}),"\n",(0,i.jsx)(n.li,{children:"Implement PDDL planner"}),"\n",(0,i.jsx)(n.li,{children:"Create task execution monitor"}),"\n",(0,i.jsx)(n.li,{children:"Test end-to-end command execution"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"milestone-4-manipulation-weeks-7-8",children:"Milestone 4: Manipulation (Weeks 7-8)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deliverables:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Basic manipulation capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Grasping and placing functionality"}),"\n",(0,i.jsx)(n.li,{children:"Integration with perception and planning"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tasks:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Implement basic manipulation"}),"\n",(0,i.jsx)(n.li,{children:"Integrate with perception system"}),"\n",(0,i.jsx)(n.li,{children:"Test pick-and-place operations"}),"\n",(0,i.jsx)(n.li,{children:"Combine with navigation and planning"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"milestone-5-full-integration-and-testing-weeks-9-10",children:"Milestone 5: Full Integration and Testing (Weeks 9-10)"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deliverables:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Complete autonomous agent"}),"\n",(0,i.jsx)(n.li,{children:"Multi-step task execution"}),"\n",(0,i.jsx)(n.li,{children:"Error handling and recovery"}),"\n",(0,i.jsx)(n.li,{children:"Performance evaluation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Tasks:"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Integrate all subsystems"}),"\n",(0,i.jsx)(n.li,{children:"Implement error handling"}),"\n",(0,i.jsx)(n.li,{children:"Test complex multi-step tasks"}),"\n",(0,i.jsx)(n.li,{children:"Evaluate performance and robustness"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"evaluation-criteria",children:"Evaluation Criteria"}),"\n",(0,i.jsx)(n.h3,{id:"technical-requirements-70",children:"Technical Requirements (70%)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Integration (15%)"}),": Proper use of ROS 2 concepts, message passing, and architecture"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Perception System (15%)"}),": Effective use of VLM for object detection and scene understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Planning System (15%)"}),": LLM-guided task planning and decomposition"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Navigation (10%)"}),": Reliable navigation in complex environments"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Manipulation (10%)"}),": Successful object grasping and manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulation Integration (5%)"}),": Proper simulation-to-reality considerations"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"performance-metrics-20",children:"Performance Metrics (20%)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Task Success Rate"}),": Percentage of successfully completed tasks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Execution Time"}),": Efficiency of task completion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Ability to handle unexpected situations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Safety"}),": Proper safety protocols and emergency procedures"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"documentation-and-presentation-10",children:"Documentation and Presentation (10%)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Code Quality"}),": Well-documented, maintainable code"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"System Documentation"}),": Clear architecture and design documentation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Final Presentation"}),": Comprehensive demonstration of capabilities"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"safety-considerations",children:"Safety Considerations"}),"\n",(0,i.jsx)(n.h3,{id:"physical-safety",children:"Physical Safety"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Implement emergency stop functionality"}),"\n",(0,i.jsx)(n.li,{children:"Set appropriate velocity and torque limits"}),"\n",(0,i.jsx)(n.li,{children:"Include collision detection and avoidance"}),"\n",(0,i.jsx)(n.li,{children:"Use proper safety-rated hardware interfaces"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"data-safety",children:"Data Safety"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Secure API key management for LLM services"}),"\n",(0,i.jsx)(n.li,{children:"Privacy considerations for camera data"}),"\n",(0,i.jsx)(n.li,{children:"Proper error handling to prevent system crashes"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"operational-safety",children:"Operational Safety"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Gradual deployment from simulation to reality"}),"\n",(0,i.jsx)(n.li,{children:"Comprehensive testing before physical deployment"}),"\n",(0,i.jsx)(n.li,{children:"Human supervision during initial real-world testing"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"resources-and-references",children:"Resources and References"}),"\n",(0,i.jsx)(n.h3,{id:"required-software",children:"Required Software"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ROS 2 Humble Hawksbill or later"}),"\n",(0,i.jsx)(n.li,{children:"Gazebo Garden or Isaac Sim"}),"\n",(0,i.jsx)(n.li,{children:"Python 3.8+ with appropriate libraries"}),"\n",(0,i.jsx)(n.li,{children:"OpenAI API access (or alternative LLM service)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"recommended-hardware-for-physical-deployment",children:"Recommended Hardware (for physical deployment)"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Humanoid robot platform (e.g., NAO, Pepper, or custom platform)"}),"\n",(0,i.jsx)(n.li,{children:"RGB-D camera"}),"\n",(0,i.jsx)(n.li,{children:"LiDAR sensor"}),"\n",(0,i.jsx)(n.li,{children:"Computing platform capable of running LLMs"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This capstone project provides a comprehensive challenge that integrates all aspects of humanoid robotics development, from low-level control to high-level AI planning, preparing students for real-world robotics applications."})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var s=t(6540);const i={},o=s.createContext(i);function a(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);